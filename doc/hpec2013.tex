\documentclass[conference]{IEEEtran}
\usepackage{listings}
\begin{document}

\title{Novel Algebras for Advanced Analytics in Julia}

\author{\IEEEauthorblockN{
Viral B. Shah\IEEEauthorrefmark{1},
Alan Edelman\IEEEauthorrefmark{2},
Stefan Karpinski\IEEEauthorrefmark{3},
Jeff Bezanson\IEEEauthorrefmark{4}, 
Jeremy Kepner\IEEEauthorrefmark{5}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Email: viral@mayin.org}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Email: edelman@math.mit.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Email: stefan@karpinski.org}
\IEEEauthorblockA{\IEEEauthorrefmark{4}Email: bezanson@mit.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{5}Email: kepner@ll.mit.edu}}
% make the title area
\maketitle

\begin{abstract}
%\boldmath
A linear algebraic approach to graph algorithms that exploits the
sparse adjacency matrix representation of graphs can provide a variety
of benefits. These benefits include syntactic simplicity, easier
implementation, and higher performance.  One way to employ linear
algebra techniques for graph algorithms is to use a broader definition
of matrix and vector multiplication.  We demonstrate through the use
of the Julia language system how easy it is to explore semirings using
linear algebraic methodologies.

\end{abstract}

\section{Introduction}

\subsection{Semiring algebra}

The duality between the canonical representation of graphs as abstract
collections of vertices and edges and a sparse adjacency matrix
representation has been a part of graph theory since its inception
~\cite{Konig1931}, ~\cite{Konig1936}. Matrix algebra has been
recognized as a useful tool in graph theory for nearly as long
(see~\cite{Harary1969} and references therein).  A linear algebraic
approach to graph algorithms that exploits the sparse adjacency matrix
representation of graphs can provide a variety of benefits. These
benefits include syntactic simplicity, easier implementation, and
higher performance.  One way to employ linear algebra techniques for
graph algorithms is to use a broader definition of matrix and vector
multiplication. One such broader definition is that of a semiring.
For example, in semiring notation we write matrix-matrix multiply as:

$$C = A +.* B$$

where $$+.*$$ denotes standard matrix multiply.  In such notation, a
semiring requires that addition and multiplication are associative,
addition is commutative, and multiplication distributes over addition
from both left and right.  In graph algorithms, a fundamental
operation is matrix-matrix multiply where both matrices are sparse.
This operation represents multi source 1-hop breadth first search
(BFS) and combine, which is the foundation of many graph
algorithms~\cite{shahgilbert}.  In addition, it is often the case that
operations other than standard matrix multiply are desired, for
example:

\begin{enumerate}
\item MaxPlus: $C = A \ {\max.+} B$
\item MinMax: $C = A \ \min.\max B$
\item OrAnd: $C = A \ |.\& B $
\item General ($f$ and $g$) : $C = A f.g B $
\end{enumerate}

With this more general case of sparse matrix multiply, a wide range of
graphs algorithms can be implemented~\cite{KepnerGilbertBook}.

\section{Application example}

A classic example of the utility of the semiring approach is in
finding the minimum path between all vertices in graph (see
~\cite{Rader} in ~\cite{KepnerGilbertBook}.  Given a weighted
adjacency matrix for a graph where $A(i,j) = w_{ij}$ is the weight of
a directed edge from vertex $i$ to vertex $j$.  Let $C(i,j)_2$ be the
minimum 2-hop cost from vertex $i$ to vertex $j$.
$C_2$ can be computed via the semiring matrix product:

$$C_2 = A \ \min.+ A$$

Likewise, $C_3$ can be computed via

$$C_3 = A\  \min.+ A \min.+ A$$

and more generally

$$C_k = A^k$$

\section{Julia}

It has become clear that programmers and scientists prefer high-level, interactive, dynamic environments for algorithm development and data analysis.
Systems such as Matlab~\cite{matlab}, Octave~\cite{Octave}, R~\cite{Rlang}, SciPy~\cite{numpy}, and SciLab~\cite{scilab} provide greatly increased convenience and productivity, yet C and Fortran remain the gold standard for computationally-intensive problems because these high-level dynamic systems still lag significantly in performance.
As a result, the most challenging areas of technical computing have benefited the least from the increased abstraction and productivity offered by higher level languages.
Julia~\cite{julia} is a high-level, dynamic language, designed from the ground up to take advantage of modern techniques for executing dynamic languages efficiently.
As a result, Julia has the performance of a statically compiled language while providing the interactive, dynamic experience and productivity that scientists have come to expect.

Julia also introduces many powerful computer science tools to scientific computing, including a sophisticated type system, multiple dispatch, coroutines, Lisp-style metaprogramming (including real macros), and built-in support for distributed computation.
Although a powerful type system is made available to the programmer, it remains unobtrusive in the sense that one is never required to specify types, nor is performance dependent upon doing so:
unless the programmer wants to take advantage of Julia's dispatch system or create C-compatible types, their code will work just as well without ever mentioning types.
Likewise, coroutines, macros and distributed programming primitives are right there, should the programmer ever need them, but are not required for day-to-day programming.

We recommend trying the code examples in this paper.
Just a few moments may convince the reader that Julia is simple yet powerful.
All of the code may be found on GitHub:
{\tt github.com/ViralBShah/SemiringAlgebra.jl}


Julia is a novel new paradigm constructed with high performance computing in mind.  Roughly speaking, high performance computing has been significantly difficult that getting codes up and working and in production has taken so much time, that there all too often has been little time for algorithmic exploration or software experimentation.  Julia is a game changer for high performance computing; but that is not the focus of this work.  In this work, our goal is to demonstrate the expressiveness and utility of Julia in the context of semirings.  We invite readers to imagine what a semiring implementation might look like, how long it would take to implement, and  how it might perform using their favorite programming methodology.
 
\section{Semiring algebra in Julia}

Julia's ordinary matrix multiplication is recognizable to users of many
high level languages: \\
{\tt A=rand(m,n) \\  B=rand(n,p) \\ C=A*B } \\
or one can equally well use the prefix notation \\
{\tt C = *(A,B)} \\
instead of the infix notation.

We believe that users of matrix-multiply who are used
to such compact expressions would prefer not to have
overly complicated syntax to express the more general
semirings.  Julia offers two approaches that are readily
available for exploration:

1) Star overloading:
This method is recommended for interactive exploration of many semiring operators.
Without introducing any types, define  {\tt *(f,g)(A,B)} to
perform the semiring operation very generally, with no a-priori restriction
on the binary functions {\tt f} or {\tt g}.   Upon overloading the star operator,
and defining the ringmatmul, the following immediately work in Julia:
\begin{verbatim}
*(max,+)(A,B)
*(min,max)(A,B)
*(|,&)(A,B)
*(+,*)(A,B)
\end{verbatim}
The last example computes the usual matrix product.

2) Creation of semiring objects:
This method is recommended for users who are working exclusively in one semiring and wish to optimize notation and performance.
In this method, a semiring type is created, and one overloads scalar {\tt +} and scalar {\tt *} only.
Julia's generic definitions for matrix multiplication, which depends only on having appropriate definitions for {\tt +} and {\tt *} does the rest, allowing one to immediately compute matrix products in the newly defined semiring, using the usual notation for matrix products.

\subsection{Star Overloading}

\begin{figure}
\begin{lstlisting}[language=matlab, frame=single]

function ringmatmul(+, *, 
                    A::Matrix,B::Matrix)
  m, p = size(A); n = size(B,2)
  C = [A[i,1]*B[1,j] for i=1:m, j=1:n]
  for i=1:m, j=1:n, k=2:p
    C[i,j] += A[i,k]*B[k,j]
  end
  return C
end

*((+)::Function, (*)::Function) = 
              (A,B)->ringmatmul(+,*,A,B)
             
\end{lstlisting}
\label{fig:ringmm}
\caption{SemiRing matrix multiply by overloading + and *.  This seemingly textbook
matrix multiply routine is anything but ordinary because + and * are local variables.  Inside the subroutine + and * take on any semiring operations with which they are called.
Thanks to Julia internals,
users may execute this code at the prompty by simply typing  {\tt *(max,+)(A,B)} or
the more familiar infix notation {\tt A *(max,+)B} . \newline
Overloading allows users to conveniently explore multiple semiring operations on the same data.
}
\end{figure}

First time users should readily be able to google, download, and
install Julia.  The star overloading functionality may be explored by
typing the example in Figure~\ref{fig:ringmm} directly into a Julia
session. The {\tt ringmatmul} function is straightforward -- it is a
standard na\"ive triple loop matrix multiply.  The function takes four
arguments, the first two, {\tt +} and {\tt *}, are functions that will
serves as the local versions of addition and multiplication, while the
last two are the matrices to multiply using those operations.  This
highlights a few relevant features of the Julia language.  First,
functions are first-class values that can be passed into other
functions as arguments and then used with standard function call
syntax.  Second, the {\tt +} and {\tt *} operators are just regular
functions with some special syntax.  For example, the expression {\tt
  C[i,j] += A[i,k]*B[k,j]} is translated into {\tt C[i,j] =
  +(C[i,j],*(A[i,k],B[k,j]))} where the functions {\tt +} and {\tt *}
are looked up in the current scope just like any other variables would
be.  The second function definition says that if ``*" is called with
two arguments, both of which have the type {\tt Function}, then the
result is a function itself which takes two arrays as arguments, and
calls {\tt ringmatmul} with arguments {\tt f,g,A,B}.  This highlights
a few more features of Julia: it uses multiple dispatch by proding a
new behavior for {\tt *} when its areguments are functions; it returns
a function as a value, which can then be used elsewhere, again with
the standard function call syntax.

\subsection{Semiring Objects}

\begin{figure}
\begin{lstlisting}[language=c++, frame=single]
immutable MPNumber{T} <: Number
    val::T
end

+(a::MPNumber,b::MPNumber)
      = MPNumber(max(a.val,b.val))
*(a::MPNumber,b::MPNumber)
      = MPNumber(a.val+b.val)
show(io::IO, k::MPNumber)
      = print(io, k.val)

zero{T}(::Type{MPNumber{T}})
      = MPNumber(typemin(T))
one{T}(::Type{MPNumber{T}})
      = MPNumber(zero(T))
promote_rule{T<:Number}(::Type{MPNumber},
                        ::Type{T})
      = MPNumber

mparray(A::Array) = map(MPNumber, A)
array{T}(A::Array{MPNumber{T}})
      = map(x->x.val, A)
mpsparse(S::SparseMatrixCSC)
      = SparseMatrixCSC(S.m, S.n, S.colptr,
             S.rowval,mparray(S.nzval))
\end{lstlisting}
\label{fig:MP}
\caption{
An altnerative approach to that of Figure 1 is the 
MPNumber type with Max-Plus Algebra properties.  Comparing with the overloading
approach, this approach automatically works with dense and sparse matrices of any type without requiring the user to redefine matrix multiply.
The code sets up a MaxPlus number
({\tt MPNumber} ), defines a plus and times operator {\tt max} and {\tt +}, sets ut the identity elements ({\tt zero=typemin{T}} and {\tt one=zero(T)}). \newline
Users simply type {\tt A*B} on arrays of the right type, and the semiring operation will just work. (See Fig. 3)
}
\end{figure}

Here we create a ``max-plus'' semiring type.  Other semirings can be implemented
by changing the definitions of ``+'' and ``*'', which are set to use
``max'' and ``plus'' (Figure~\ref{fig:MP}).
The new type does not need to be taught matrix multiply; matmul will work with
any underlying numeric type (e.g. int, float, ...), and will also work with
both dense and sparse matrices. Arrays of the new semiring type will use the
same amount of memory as ``primitive'' arrays of the underlying numeric type.

The first definition describes the \verb+MPNumber+ type.
Then plus and times are defined as max and plus respectively, followed by
a routine IO function.

The zero and one in this semiring are defined as the identity elements for max and plus,
and the promote rule is a Julia construction which allows semiring numbers and ordinary
numbers to work together.

Finally the last three functions allow for the conversion between ordinary arrays and the semiring, with a special extra constructor for the important use case of sparse arrays.

Some examples of using the code are presented in the figure below:

\begin{figure}[h]
\begin{lstlisting}[language=python, frame=single]
# Random MPNumber array
A=mparray(rand(3,3)) 
B=mparray(rand(3,3))

# Multiply two matrices
A*B

# Square a matrix
A^2

# Create a sparse identity matrix
C=mpsparse(sparse(eye(3,3)))

# Square a sparse matrix
C*C
\end{lstlisting}
\label{fig:example}
\caption{Example creation of an {\tt mparray}.
Users simply create an mparray, and operate normally from there.
The operations {\tt *} and $\wedge$  are automatically the max-plus operations without any user definition.}
\end{figure}

\section{Performance}

Pne.
The goal of Julia is to have the right combination of reasonable performance
for the machine and productivity for the human.  
On a Macbook Pro, with dual-core 2.4 GHz Intel core i5, 8GB RAM, we compared a dense BLAS run of matmul with an MPNumber\{Float64\} run in the semiring.
The BLAS time was 0.26msec (practically free!) for a $100 \times 100$ array.  The semiring time
was 313 msec.  This is no surprise.

More interesting and more useful is the comparison for sparse
matrices.  For an $n \times n$ sparse random array with density $1/n$,
where $n = 100,000$ the ordinary matrix multiply took 21 msec, and the
semiring matrix multiply took a comparable 22 msec. For a higher
density of $5/n$, we found that the ordinary sparse matrix
multiplication again was on par with the semiring version, taking 450
msec in each case.


\section{Conclusion}

Julia facilitates the implementation and exploration of graph
algorithms through the semiring notation of generalized matrix
mutliply.  These algorithms appear in applications to data analysis
and related fields.  Exploitation of sparse data strucutres provides
easy implementations with reasonable performance.  Sparse semiring
performance is comparable to ordinary sparse matmul in the Julia
language.

\section{Acknowledgements}

We gratefully acknowledge funding from Citigroup, Intel ISTC, VMware, and the Deshpande Foundation.  The other authors thank Jeremy Kepner for raising our awareness of the importance of semirings in high performance computing and data analysis.  

\newpage
\bibliographystyle{plain}
\bibliography{hpec2013}

\end{document}
